# PromptShield

> a lightweight jailbreak detection system for Large Language Models (LLMs), fine-tuned using efficient LoRA (Low-Rank Adaptation) techniques. It classifies prompts as safe or potentially adversarial (jailbreaks) in real-time, enabling safer LLM deployments. 
